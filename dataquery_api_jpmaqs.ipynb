{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import concurrent.futures, logging, os, json\n",
    "    from datetime import datetime as datetime, timedelta, timezone\n",
    "    import time, functools\n",
    "    from typing import Dict, Generator, Iterable, List, Optional, Union, overload\n",
    "    import pandas as pd, requests, requests.compat\n",
    "    from tqdm import tqdm\n",
    "except ImportError as e:\n",
    "    print(f\"Import Error: {e}\")\n",
    "    print(\"Please install the required packages in your Python environment using the following command:\")\n",
    "    print(\"\\n\\t python -m pip install pandas requests tqdm\\n\")\n",
    "    raise e\n",
    "logger = logging.getLogger(__name__)\n",
    "OAUTH_BASE_URL = \"https://api-developer.jpmorgan.com/research/dataquery-authe/api/v2\"\n",
    "TIMESERIES_ENDPOINT = \"/expressions/time-series\"\n",
    "HEARTBEAT_ENDPOINT = \"/services/heartbeat\"\n",
    "CATALOGUE_ENDPOINT = \"/group/instruments\"\n",
    "OAUTH_TOKEN_URL = \"https://authe.jpmchase.com/as/token.oauth2\"\n",
    "OAUTH_DQ_RESOURCE_ID = \"JPMC:URI:RS-06785-DataQueryExternalApi-PROD\"\n",
    "API_DELAY_PARAM = 0.2\n",
    "TOKEN_EXPIRY_BUFFER = 0.9\n",
    "EXPR_LIMIT = 20\n",
    "JPMAQS_GROUP_ID = \"JPMAQS\"\n",
    "MAX_RETRY = 3\n",
    "\n",
    "\n",
    "def form_full_url(url, params={}):\n",
    "    return requests.compat.quote(f\"{url}?{requests.compat.urlencode(params)}\" if params else url, safe=\"%/:=&?~#+!$,;'@()*[]\")\n",
    "\n",
    "\n",
    "def construct_jpmaqs_expressions(ticker, metrics=[\"value\", \"grading\", \"eop_lag\", \"mop_lag\"]):\n",
    "    if isinstance(ticker, str):\n",
    "        return [f\"DB(JPMAQS,{ticker},{metric})\" for metric in metrics]\n",
    "    return [f\"DB(JPMAQS,{t},{metric})\" for t in ticker for metric in metrics]\n",
    "\n",
    "\n",
    "def time_series_to_df(dicts_list):\n",
    "    if isinstance(dicts_list, dict):\n",
    "        dicts_list = [dicts_list]\n",
    "    expressions = [d[\"attributes\"][0][\"expression\"] for d in dicts_list]\n",
    "    return_df = pd.concat(\n",
    "        [pd.DataFrame(dicts_list.pop()[\"attributes\"][0][\"time-series\"], columns=[\"real_date\", \"value\"]).assign(expression=expressions.pop()) for _ in range(len(dicts_list))],\n",
    "        axis=0,\n",
    "    ).reset_index(\n",
    "        drop=True\n",
    "    )[[\"real_date\", \"expression\", \"value\"]]\n",
    "    return_df[\"real_date\"] = pd.to_datetime(return_df[\"real_date\"])\n",
    "    return return_df\n",
    "\n",
    "\n",
    "def request_wrapper(url, headers=None, params=None, method=\"get\", **kwargs):\n",
    "    try:\n",
    "        response = requests.request(method=method, url=url, params=params, headers=headers, **kwargs)\n",
    "        if response.status_code == 200:\n",
    "            return response\n",
    "        else:\n",
    "            raise Exception(f\"Request failed with status code {response.status_code}.\\nTimestamp (UTC): {datetime.now(timezone.utc).isoformat()}\\nResponse : {response.text}\\nURL: {form_full_url(url,params)}Request headers: {headers}\\n\")\n",
    "    except Exception as e:\n",
    "        if isinstance(e, requests.exceptions.ProxyError):\n",
    "            raise Exception(\"Proxy error. Check your proxy settings. Exception : \", e)\n",
    "        elif isinstance(e, requests.exceptions.ConnectionError):\n",
    "            raise Exception(\"Connection error. Check your internet connection. Exception : \", e)\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "\n",
    "class DQInterface:\n",
    "    def __init__(self, client_id, client_secret, proxy=None, base_url=OAUTH_BASE_URL, dq_resource_id=OAUTH_DQ_RESOURCE_ID):\n",
    "        self.client_id = client_id\n",
    "        self.client_secret = client_secret\n",
    "        self.proxy = proxy\n",
    "        self.dq_resource_id = dq_resource_id\n",
    "        self.current_token = None\n",
    "        self.base_url = base_url\n",
    "        self.token_data = {\n",
    "            \"grant_type\": \"client_credentials\",\n",
    "            \"client_id\": self.client_id,\n",
    "            \"client_secret\": self.client_secret,\n",
    "            \"aud\": self.dq_resource_id,\n",
    "        }\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args, **kwargs): ...\n",
    "    def get_access_token(self):\n",
    "        def _is_active(token=None):\n",
    "            if token is None:\n",
    "                return False\n",
    "            expires = token[\"created_at\"] + timedelta(seconds=token[\"expires_in\"] * TOKEN_EXPIRY_BUFFER)\n",
    "            return datetime.now() < expires\n",
    "\n",
    "        if _is_active(self.current_token):\n",
    "            return self.current_token[\"access_token\"]\n",
    "        else:\n",
    "            r_json = request_wrapper(url=OAUTH_TOKEN_URL, data=self.token_data, method=\"post\", proxies=self.proxy).json()\n",
    "            self.current_token = {\"access_token\": r_json[\"access_token\"], \"created_at\": datetime.now(), \"expires_in\": r_json[\"expires_in\"]}\n",
    "            return self.current_token[\"access_token\"]\n",
    "\n",
    "    def _request(self, url, params, **kwargs):\n",
    "        return request_wrapper(url=url, params=params, headers={\"Authorization\": f\"Bearer {self.get_access_token()}\"}, method=\"get\", proxies=self.proxy, **kwargs).json()\n",
    "\n",
    "    def heartbeat(self, raise_error=False):\n",
    "        url = self.base_url + HEARTBEAT_ENDPOINT\n",
    "        response = self._request(url=url, params={\"data\": \"NO_REFERENCE_DATA\"})\n",
    "        result = \"info\" in response\n",
    "        if not result and raise_error:\n",
    "            raise Exception(f\"DataQuery API Heartbeat failed. \\n Response : {response} \\nUser ID: {self.get_access_token()['user_id']}\\nTimestamp (UTC): {datetime.now(timezone.utc).isoformat()}\")\n",
    "        return result\n",
    "\n",
    "    def _fetch(self, url, params, **kwargs):\n",
    "        downloaded_data = []\n",
    "        response = self._request(url=url, params=params, **kwargs)\n",
    "        if response is None or \"instruments\" not in response.keys():\n",
    "            if response is not None:\n",
    "                if \"info\" in response and \"code\" in response[\"info\"] and int(response[\"info\"][\"code\"]) == 204:\n",
    "                    raise Exception(f\"Content was not found for the request: {response}\\nUser ID: {self.get_access_token()['user_id']}\\nURL: {form_full_url(url,params)}\\nTimestamp (UTC): {datetime.now(timezone.utc).isoformat()}\")\n",
    "            raise Exception(f\"Invalid response from DataQuery: {response}\\nUser ID: {self.get_access_token()['user_id']}\\nURL: {form_full_url(url,params)}Timestamp (UTC): {datetime.now(timezone.utc).isoformat()}\")\n",
    "        downloaded_data.extend(response[\"instruments\"])\n",
    "        if \"links\" in response.keys() and response[\"links\"][1][\"next\"] is not None:\n",
    "            downloaded_data.extend(self._fetch(url=self.base_url + response[\"links\"][1][\"next\"], params={}, **kwargs))\n",
    "        return downloaded_data\n",
    "\n",
    "    def get_catalogue(self, group_id=JPMAQS_GROUP_ID, verbose=True, show_progress=True):\n",
    "        if verbose:\n",
    "            print(f\"Downloading the {group_id} catalogue from DataQuery...\")\n",
    "        try:\n",
    "            response_list = self._fetch(url=self.base_url + CATALOGUE_ENDPOINT, params={\"group-id\": group_id})\n",
    "            if show_progress:\n",
    "                print()\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        tickers = [d[\"instrument-name\"] for d in response_list]\n",
    "        utkr_count = len(tickers)\n",
    "        tkr_idx = sorted([d[\"item\"] for d in response_list])\n",
    "        if not (min(tkr_idx) == 1 and max(tkr_idx) == utkr_count and len(set(tkr_idx)) == utkr_count):\n",
    "            raise ValueError(\"The downloaded catalogue is corrupt.\")\n",
    "        return tickers\n",
    "\n",
    "    def _get_result(self, url, params, save_to_path, **kwargs):\n",
    "        timeseries_list = self._fetch(url, params, **kwargs)\n",
    "        if save_to_path is None:\n",
    "            return timeseries_list\n",
    "        if not os.path.exists(save_to_path):\n",
    "            os.makedirs(save_to_path, exist_ok=True)\n",
    "        results = []\n",
    "        while len(timeseries_list) > 0:\n",
    "            ts = timeseries_list.pop(0)\n",
    "            if ts[\"attributes\"][0][\"time-series\"] is None:\n",
    "                continue\n",
    "            expr = ts[\"attributes\"][0][\"expression\"]\n",
    "            pth = os.path.join(save_to_path, f\"{expr}.csv\")\n",
    "            pd.DataFrame(ts[\"attributes\"][0][\"time-series\"], columns=[\"real_date\", \"value\"]).dropna().to_csv(pth, index=False)\n",
    "            results.append(pth)\n",
    "        return results\n",
    "\n",
    "    def _get_timeseries(self, expressions, params, as_dataframe=True, save_to_path=None, max_retry=MAX_RETRY, show_progress=True, **kwargs):\n",
    "        if max_retry < 0:\n",
    "            raise Exception(\"Maximum number of retries reached.\")\n",
    "        expr_batches = [[expressions[i : min(i + EXPR_LIMIT, len(expressions))]] for i in range(0, len(expressions), EXPR_LIMIT)]\n",
    "        downloaded_data = []\n",
    "        failed_batches = []\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = []\n",
    "            for expr_batch in tqdm(expr_batches, desc=\"Requesting data\", disable=not show_progress, total=len(expr_batches)):\n",
    "                current_params = params.copy()\n",
    "                current_params[\"expressions\"] = expr_batch\n",
    "                curr_url = self.base_url + TIMESERIES_ENDPOINT\n",
    "                futures.append(executor.submit(self._get_result, url=curr_url, params=current_params, save_to_path=save_to_path))\n",
    "                time.sleep(API_DELAY_PARAM)\n",
    "            for ix, future in tqdm(enumerate(futures), desc=\"Downloading data\", disable=not show_progress, total=len(futures)):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if save_to_path is not None:\n",
    "                        if not all(result):\n",
    "                            raise Exception(f\"Failed to save data to path `{save_to_path}` for batch {ix}.\")\n",
    "                    downloaded_data.extend(result)\n",
    "                except Exception as e:\n",
    "                    failed_batches.append(expr_batches[ix])\n",
    "                    logger.error(f\"Failed to download data for batch {ix} : {e}\")\n",
    "        if len(failed_batches) > 0:\n",
    "            retry_exprs = [expr for batch in failed_batches for expr in batch]\n",
    "            if max_retry > 0:\n",
    "                print(f\"Retrying failed expressions: {retry_exprs};\", f\"\\nRetries left: {max_retry}\")\n",
    "                return self._get_timeseries(\n",
    "                    expressions=retry_exprs,\n",
    "                    params=params,\n",
    "                    as_dataframe=as_dataframe,\n",
    "                    save_to_path=save_to_path,\n",
    "                    max_retry=max_retry - 1,\n",
    "                    show_progress=show_progress,\n",
    "                    **kwargs,\n",
    "                )\n",
    "            else:\n",
    "                print(f\"Failed to download data for expressions: {retry_exprs}\", \"\\nMaximum number of retries reached, skipping failed expressions.\")\n",
    "                return []\n",
    "        return downloaded_data\n",
    "\n",
    "    def download(\n",
    "        self,\n",
    "        expressions,\n",
    "        start_date,\n",
    "        end_date,\n",
    "        as_dataframe=True,\n",
    "        path=None,\n",
    "        show_progress=False,\n",
    "        calender=\"CAL_WEEKDAYS\",\n",
    "        frequency=\"FREQ_DAY\",\n",
    "        conversion=\"CONV_LASTBUS_ABS\",\n",
    "        nan_treatment=\"NA_NOTHING\",\n",
    "    ):\n",
    "        if path is not None:\n",
    "            path = os.path.expanduser(path)\n",
    "            os.makedirs(os.path.normpath(path), exist_ok=True)\n",
    "        if end_date is None:\n",
    "            end_date = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n",
    "        params_dict = {\n",
    "            \"format\": \"JSON\",\n",
    "            \"start-date\": start_date,\n",
    "            \"end-date\": end_date,\n",
    "            \"calendar\": calender,\n",
    "            \"frequency\": frequency,\n",
    "            \"conversion\": conversion,\n",
    "            \"nan_treatment\": nan_treatment,\n",
    "            \"data\": \"NO_REFERENCE_DATA\",\n",
    "        }\n",
    "        dwnld_start = time.time()\n",
    "        if self.heartbeat(raise_error=True):\n",
    "            print(f\"Timestamp (UTC): {datetime.now(timezone.utc).isoformat()}\")\n",
    "            print(\"Connected to DataQuery API!\")\n",
    "        downloaded_data = self._get_timeseries(expressions=expressions, params=params_dict, as_dataframe=as_dataframe, save_to_path=path, show_progress=show_progress)\n",
    "        dwnld_end = time.time()\n",
    "        print(f\"Download done.\\nTimestamp (UTC): {datetime.now(timezone.utc).isoformat()}.\\nDownload took {dwnld_end-dwnld_start:.2f} seconds.\")\n",
    "        if path:\n",
    "            print(f\"Data saved to {path}.\")\n",
    "            print(f\"Downloaded {len(downloaded_data)} / {len(expressions)} files.\")\n",
    "            result = [{\"expression\": str(os.path.basename(f)).split(\".\")[0], \"file\": str(os.path.abspath(os.path.normpath(f))).replace(\"\\\\\", \"/\")} for f in downloaded_data]\n",
    "            logger.info(f\"Data saved to {path}.\")\n",
    "            logger.info(f\"Saved files: {result}\")\n",
    "            return result\n",
    "        mismm = \"Expression not found; No message available.\"\n",
    "        missing_exprs = [(expr[\"attributes\"][0][\"expression\"], expr[\"attributes\"][0].get(\"message\", mismm)) for expr in downloaded_data if expr[\"attributes\"][0][\"time-series\"] is None]\n",
    "        if len(missing_exprs) > 0:\n",
    "            logger.warning(f\"Missing expressions: {missing_exprs}\")\n",
    "            print(f\"Missing expressions: {missing_exprs}\\nDownloaded {len(downloaded_data)-len(missing_exprs)} / {len(expressions)} expressions.\")\n",
    "            downloaded_data = [expr for expr in downloaded_data if expr[\"attributes\"][0][\"time-series\"] is not None]\n",
    "        if as_dataframe:\n",
    "            return time_series_to_df(downloaded_data)\n",
    "        return downloaded_data\n",
    "\n",
    "\n",
    "def concat_csvs_to_df(real_date=\"real_date\", df_paths=[], metrics=[], path=str):\n",
    "    functools.reduce(\n",
    "        lambda x, y: pd.merge(x, y, on=real_date, how=\"outer\"),\n",
    "        [pd.read_csv(f, parse_dates=[real_date]).rename(columns={\"value\": metric}).set_index(real_date) for (f, metric) in zip(df_paths, metrics)],\n",
    "    ).sort_values(\n",
    "        by=real_date\n",
    "    ).reset_index().to_csv(path, index=False)\n",
    "\n",
    "\n",
    "def cleanup_csvs(path, expressions_paths):\n",
    "    splitexpr = lambda s: str(s).replace(\"DB(JPMAQS,\", \"\").replace(\")\", \"\").split(\",\")\n",
    "    getticker = lambda s: splitexpr(s)[0]\n",
    "    getmetric = lambda s: splitexpr(s)[1]\n",
    "    getcid = lambda s: getticker(s).split(\"_\")[0]\n",
    "    getxcat = lambda s: getticker(s).split(\"_\", 1)[1]\n",
    "    all_expr_for_ticker = lambda t: list(filter(lambda d: getticker(d[\"expression\"]) == t, expressions_paths))\n",
    "    tickers_all = list(set([getticker(d[\"expression\"]) for d in expressions_paths]))\n",
    "    for ticker in tqdm(tickers_all, desc=\"Formatting CSVs\"):\n",
    "        xc_path = os.path.join(path, getxcat(ticker))\n",
    "        os.makedirs(xc_path, exist_ok=True)\n",
    "        exprs = all_expr_for_ticker(ticker)\n",
    "        if len(exprs) == 0:\n",
    "            continue\n",
    "        df_paths = [d[\"file\"] for d in exprs]\n",
    "        metrics = [getmetric(d[\"expression\"]) for d in exprs]\n",
    "        ticker_path = os.path.join(xc_path, f\"{ticker}.csv\")\n",
    "        functools.reduce(\n",
    "            lambda x, y: pd.merge(x, y, on=\"real_date\", how=\"outer\"),\n",
    "            [pd.read_csv(f, parse_dates=[\"real_date\"]).rename(columns={\"value\": metric}).set_index(\"real_date\") for (f, metric) in zip(df_paths, metrics)],\n",
    "        ).sort_values(\n",
    "            by=\"real_date\"\n",
    "        ).reset_index().to_csv(ticker_path, index=False)\n",
    "        for f in df_paths:\n",
    "            os.remove(f)\n",
    "\n",
    "\n",
    "def download_all_jpmaqs_to_disk(client_id, client_secret, proxy=None, path=\"./data\", show_progress=False, start_date=\"1990-01-01\", end_date=None):\n",
    "    if not isinstance(path, str):\n",
    "        raise ValueError(\"`path` must be a string.\")\n",
    "    path = os.path.join(os.path.expanduser(path), \"JPMaQSDATA\").replace(\"\\\\\", \"/\")\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "    data = []\n",
    "    tickers = []\n",
    "    with DQInterface(client_id=client_id, client_secret=client_secret, proxy=proxy) as dq:\n",
    "        tickers = dq.get_catalogue()\n",
    "        expressions = construct_jpmaqs_expressions(tickers)\n",
    "        data = dq.download(expressions=expressions, start_date=start_date, end_date=end_date, path=path, show_progress=show_progress)\n",
    "    rmexpr = lambda s: str(s).replace(\"DB(JPMAQS,\", \"\").replace(\")\", \"\").split(\",\")[0]\n",
    "    downloaded_data = []\n",
    "    for d in data:\n",
    "        if os.path.exists(d[\"file\"]):\n",
    "            downloaded_data.append(d)\n",
    "        else:\n",
    "            print(f\"Error - File not found: Expression {d['expression']}, File {d['file']}\")\n",
    "    if len(downloaded_data) != len(data):\n",
    "        print(\"The following tickers will not have concatenated data:\")\n",
    "        print(set(tickers) - set([rmexpr(d[\"expression\"]) for d in downloaded_data]))\n",
    "    if len(downloaded_data) > 0:\n",
    "        cleanup_csvs(path, downloaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = \"your_client_id\"\n",
    "client_secret = \"your_client_secret\"\n",
    "proxy = None\n",
    "path = None\n",
    "\n",
    "expressions = [\n",
    "    \"DB(CFX,USD,)\",\n",
    "    \"DB(CFX,AUD,)\",\n",
    "    \"DB(CFX,GBP,)\",\n",
    "    \"DB(JPMAQS,USD_EQXR_VT10,value)\",\n",
    "    \"DB(JPMAQS,EUR_EQXR_VT10,value)\",\n",
    "    \"DB(JPMAQS,AUD_EXALLOPENNESS_NSA_1YMA,value)\",\n",
    "    \"DB(JPMAQS,AUD_EXALLOPENNESS_NSA_1YMA,grading)\",\n",
    "    \"DB(JPMAQS,GBP_EXALLOPENNESS_NSA_1YMA,eop_lag)\",\n",
    "    \"DB(JPMAQS,GBP_EXALLOPENNESS_NSA_1YMA,mop_lag)\",\n",
    "]\n",
    "\n",
    "\n",
    "with DQInterface(client_id=client_id, client_secret=client_secret, proxy=proxy) as dq:\n",
    "    data = dq.download(\n",
    "        expressions=expressions,\n",
    "        start_date=\"2023-02-20\",\n",
    "        end_date=\"2023-03-01\",\n",
    "        path=path,\n",
    "    )\n",
    "    if not path:\n",
    "        print(data.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = \"your_client_id\"\n",
    "client_secret = \"your_client_secret\"\n",
    "proxy = None\n",
    "path = None\n",
    "\n",
    "\n",
    "download_all_jpmaqs_to_disk(\n",
    "    client_id=\"your_client_id\",\n",
    "    client_secret=\"your_client_secret\",\n",
    "    proxy=None,\n",
    "    path=\"./data\",\n",
    "    show_progress=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
